{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import program\n",
    "from agent import Agent\n",
    "from referee.game import Board, SpawnAction, SpreadAction, HexPos, HexDir, PlayerColor, constants\n",
    "my_board = Board()\n",
    "action_1 = SpawnAction(HexPos(1,1))\n",
    "\n",
    "my_board.apply_action(action_1)\n",
    "\n",
    "# print(my_board.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取所有棋子颜色的坐标 & 空坐标，并存在list里面\n",
    "from typing import List, Optional\n",
    "\n",
    "def extract_positions(board, color: Optional[PlayerColor] = None) -> List[HexPos]:\n",
    "    positions = []\n",
    "\n",
    "    for position, cell_state in board._state.items():\n",
    "        if color is None and cell_state.player is None:  # Extract empty positions\n",
    "            positions.append(position)\n",
    "        elif cell_state.player == color:  # Extract colored positions\n",
    "            positions.append(position)\n",
    "\n",
    "    return positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 给每个棋子都设定6个方向的spread\n",
    "def generate_spread_actions(color, board) -> List[SpreadAction]:\n",
    "    colored_positions = extract_positions(board, color)\n",
    "    spread_actions = []\n",
    "\n",
    "    for position in colored_positions:\n",
    "        for direction in HexDir:\n",
    "            neighbor_pos = position + direction\n",
    "            if board._within_bounds(neighbor_pos):\n",
    "                spread_action = SpreadAction(cell=position, direction=direction)\n",
    "                spread_actions.append(spread_action)\n",
    "\n",
    "    return spread_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPAWN actions\n",
    "def generate_spawn_actions(color, board) -> List[SpawnAction]:\n",
    "    spawn_actions = []\n",
    "    empty_list = extract_positions(board)\n",
    "\n",
    "    for position in empty_list:\n",
    "        if board._within_bounds(position):\n",
    "            spawn_action = SpawnAction(cell=position)\n",
    "            spawn_actions.append(spawn_action)\n",
    "\n",
    "    return spawn_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Union\n",
    "def get_legal_actions(color, board) -> List[Union[SpawnAction, SpreadAction]]:\n",
    "    spawn_actions = generate_spawn_actions(color, board)\n",
    "    spread_actions = generate_spread_actions(color, board)\n",
    "    all_actions = spawn_actions + spread_actions\n",
    "\n",
    "    red_power = board._color_power(PlayerColor.RED)\n",
    "    blue_power = board._color_power(PlayerColor.BLUE)\n",
    "\n",
    "    if(red_power+blue_power <= 49):\n",
    "        return all_actions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_terminal(game_state):\n",
    "    return game_state.game_over"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Implement a game state evaluation function: Create a function that assigns a numerical value to a given game state, representing the desirability of the state for each agent. The evaluation function should consider the position of pieces, material balance, and other relevant factors specific to your game.\n",
    "\n",
    "2. Define the utility function: Create a utility function to determine if a game state is a terminal state (i.e., the game has ended) and return the corresponding utility value.\n",
    "\n",
    "3. Implement the minimax algorithm with alpha-beta pruning: Write a recursive function that explores the game tree up to a certain depth. The function alternates between maximizing and minimizing layers, corresponding to the Red agent's turn and the Blue agent's turn, respectively. Use alpha-beta pruning to improve efficiency by pruning branches of the game tree that do not need to be explored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_list = extract_positions(my_board, PlayerColor.RED)\n",
    "# red_list\n",
    "blue_list = extract_positions(my_board, PlayerColor.BLUE)\n",
    "# blue_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_state_and_complexity(game_state: Board):\n",
    "#     # Calculate the total power for each player\n",
    "#     red_power = game_state._color_power(PlayerColor.RED)\n",
    "#     blue_power = game_state._color_power(PlayerColor.BLUE)\n",
    "\n",
    "#     # Calculate the power difference between players\n",
    "#     power_difference = red_power - blue_power\n",
    "\n",
    "#     # Calculate the number of pieces for each player\n",
    "#     red_pieces = len(game_state._player_cells(PlayerColor.RED))\n",
    "#     blue_pieces = len(game_state._player_cells(PlayerColor.BLUE))\n",
    "\n",
    "#     # Calculate the piece difference between players\n",
    "#     piece_difference = red_pieces - blue_pieces\n",
    "\n",
    "#     empty_neighbors_total = 0\n",
    "#     empty_count = 0\n",
    "#     for pos, cell in game_state._state.items():\n",
    "#         if cell.player is not None:\n",
    "#             piece_count[cell.player] += 1\n",
    "#             power_difference[cell.player] += cell.power\n",
    "#             empty_neighbor_count[cell.player] += game_state.empty_neighbors(pos)\n",
    "    \n",
    "#     # Assign weights to the factors and compute the final score\n",
    "#     power_weight = 0.5\n",
    "#     piece_weight = 1.0\n",
    "#     empty_neighbor_weight = 0.25\n",
    "#     score = (power_weight * power_difference +\n",
    "#              piece_weight * piece_difference +\n",
    "#              empty_neighbor_weight * empty_neighbors_total)\n",
    "\n",
    "#     # Estimate complexity\n",
    "#     total_power = red_power + blue_power\n",
    "#     total_pieces = red_pieces + blue_pieces\n",
    "#     complexity = total_power * 0.5 + total_pieces * 0.5\n",
    "\n",
    "#     return score, complexity\n",
    "def evaluate_state_and_complexity(game_state: Board):\n",
    "    # Calculate the total power for each player\n",
    "    red_power = game_state._color_power(PlayerColor.RED)\n",
    "    blue_power = game_state._color_power(PlayerColor.BLUE)\n",
    "\n",
    "    # Calculate the power difference between players\n",
    "    power_difference = red_power - blue_power\n",
    "\n",
    "    # Calculate the number of pieces for each player\n",
    "    # red_cells = game_state._player_cells(PlayerColor.RED)\n",
    "    # blue_cells = game_state._player_cells(PlayerColor.BLUE)\n",
    "    red_pieces = len(game_state._player_cells(PlayerColor.RED))\n",
    "    blue_pieces = len(game_state._player_cells(PlayerColor.BLUE))\n",
    "\n",
    "    # Calculate the piece difference between players\n",
    "    piece_difference = red_pieces - blue_pieces\n",
    "\n",
    "    # # Calculate the control score\n",
    "    # red_control = sum(game_state.empty_neighbors(cell) for cell in red_cells)\n",
    "    # blue_control = sum(game_state.empty_neighbors(cell) for cell in blue_cells)\n",
    "\n",
    "    empty_neighbors_total = 0\n",
    "    for pos, cell in game_state._state.items():\n",
    "        row, col = pos\n",
    "        if row < 7 and cell.player is not None:\n",
    "            empty_neighbors_total += game_state.empty_neighbors(pos) \n",
    "\n",
    "    # Assign weights to the factors and compute the final score\n",
    "    power_weight = 0.5\n",
    "    piece_weight = 1.0\n",
    "    control_weight = 0.2\n",
    "    score = (\n",
    "        power_weight * power_difference\n",
    "        + piece_weight * piece_difference\n",
    "        + control_weight * empty_neighbors_total\n",
    "    )\n",
    "\n",
    "    # Calculate complexity\n",
    "    # achieve the Adaptive depth to Adjust the depth of your search based on the complexity of the game state\n",
    "    complexity = (red_power + blue_power) * power_weight + (red_pieces + blue_pieces) + piece_weight\n",
    "\n",
    "    return score, complexity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that adjusts the depth based on the complexity\n",
    "\n",
    "def adaptive_depth(game_state, min_depth, max_depth):\n",
    "    score, complexity = evaluate_state_and_complexity(game_state)\n",
    "\n",
    "    # You can adjust these values based on your specific requirements\n",
    "    low_complexity_threshold = 10\n",
    "    high_complexity_threshold = 20\n",
    "\n",
    "    if complexity <= low_complexity_threshold:\n",
    "        return max_depth\n",
    "    elif complexity >= high_complexity_threshold:\n",
    "        return min_depth\n",
    "    else:\n",
    "        # Linear interpolation between min_depth and max_depth\n",
    "        depth_range = max_depth - min_depth\n",
    "        complexity_range = high_complexity_threshold - low_complexity_threshold\n",
    "        depth = max_depth - ((complexity - low_complexity_threshold) / complexity_range) * depth_range\n",
    "        return int(depth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def score_action(action, game_state, maximizing_player):\n",
    "    new_state = deepcopy(game_state)\n",
    "    new_state.apply_action(action)\n",
    "    score, complexity = evaluate_state_and_complexity(game_state)\n",
    "\n",
    "    # Invert the score if it's Blue's turn (minimizing player)\n",
    "    if not maximizing_player:\n",
    "        score = -score\n",
    "\n",
    "    return score\n",
    "\n",
    "def sorted_legal_actions(color, game_state, maximizing_player):\n",
    "    legal_actions = get_legal_actions(color, game_state)\n",
    "    return sorted(legal_actions, key=lambda action: score_action(action, game_state, maximizing_player), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transposition table是一个缓存，它存储了游戏树中先前评估的位置的结果，允许搜索算法重复使用这些结果并节省时间。为了实现换位表，你可以使用Python字典来存储不同游戏状态的评估分数。\n",
    "# create a hash function to represent the game state as a unique key.\n",
    "\n",
    "# 通过使用换位表，带有α-β修剪的最小化搜索将重新使用以前探索过的游戏状态的评估分数，从而降低时间和空间的复杂性。这种优化的有效性取决于游戏树中遇到的转置的数量。\n",
    "\n",
    "import hashlib\n",
    "\n",
    "def hash_game_state(game_state):\n",
    "    # Convert the game_state._state into a sorted list of tuples\n",
    "    board_list = sorted([(cell, state.player, state.power) for cell, state in game_state._state.items()])\n",
    "\n",
    "    # Convert the sorted list of tuples into a string\n",
    "    board_string = str(board_list)\n",
    "\n",
    "    # Create a hash from the board_string\n",
    "    return hashlib.md5(board_string.encode('utf-8')).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def minimax_alpha_beta(game_state, depth, alpha, beta, maximizing_player):\n",
    "#     if depth == 0 or is_terminal(game_state):\n",
    "#         return evaluate_state(game_state)\n",
    "\n",
    "#     legal_actions = get_legal_actions(game_state._turn_color, game_state)\n",
    "\n",
    "#     if maximizing_player:\n",
    "#         max_eval = float('-inf')\n",
    "#         for action in legal_actions:\n",
    "#             game_state.apply_action(action)\n",
    "#             eval = minimax_alpha_beta(game_state, depth - 1, alpha, beta, False)\n",
    "#             game_state.undo_action()\n",
    "#             max_eval = max(max_eval, eval)\n",
    "#             alpha = max(alpha, eval)\n",
    "#             if beta <= alpha:\n",
    "#                 break\n",
    "#         return max_eval\n",
    "\n",
    "#     else:\n",
    "#         min_eval = float('inf')\n",
    "#         for action in legal_actions:\n",
    "#             game_state.apply_action(action)\n",
    "#             eval = minimax_alpha_beta(game_state, depth - 1, alpha, beta, True)\n",
    "#             game_state.undo_action()\n",
    "#             min_eval = min(min_eval, eval)\n",
    "#             beta = min(beta, eval)\n",
    "#             if beta <= alpha:\n",
    "#                 break\n",
    "#         return min_eval\n",
    "\n",
    "import time\n",
    "\n",
    "def minimax_alpha_beta(game_state, depth, alpha, beta, maximizing_player, transposition_table, start_time, time_limit):\n",
    "    # Check if the time limit has been exceeded\n",
    "    if time.time() - start_time > time_limit:\n",
    "        return None\n",
    "\n",
    "    if depth == 0 or is_terminal(game_state):\n",
    "        score, complexity = evaluate_state_and_complexity(game_state)\n",
    "        return score\n",
    "\n",
    "    game_state_hash = hash_game_state(game_state)\n",
    "\n",
    "    # Check if the game state is already in the transposition table\n",
    "    if game_state_hash in transposition_table:\n",
    "        return transposition_table[game_state_hash]\n",
    "\n",
    "    legal_actions = sorted_legal_actions(game_state._turn_color, game_state, maximizing_player)\n",
    "\n",
    "    if maximizing_player:\n",
    "        max_eval = float('-inf')\n",
    "        for action in legal_actions:\n",
    "            game_state.apply_action(action)\n",
    "            eval = minimax_alpha_beta(game_state, depth - 1, alpha, beta, False, transposition_table, start_time, time_limit)\n",
    "            game_state.undo_action()\n",
    "            if eval is None:\n",
    "                return None\n",
    "            max_eval = max(max_eval, eval)\n",
    "            alpha = max(alpha, eval)\n",
    "            if beta <= alpha:\n",
    "                break\n",
    "        transposition_table[game_state_hash] = max_eval\n",
    "        return max_eval\n",
    "\n",
    "    else:\n",
    "        min_eval = float('inf')\n",
    "        for action in legal_actions:\n",
    "            game_state.apply_action(action)\n",
    "            eval = minimax_alpha_beta(game_state, depth - 1, alpha, beta, True, transposition_table, start_time, time_limit)\n",
    "            game_state.undo_action()\n",
    "            if eval is None:\n",
    "                return None\n",
    "            min_eval = min(min_eval, eval)\n",
    "            beta = min(beta, eval)\n",
    "            if beta <= alpha:\n",
    "                break\n",
    "        transposition_table[game_state_hash] = min_eval\n",
    "        return min_eval\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Determine the best action: Call the minimax function for the current game state and depth, and store the evaluation values for all legal actions. The best action is the one with the maximum evaluation value for the Red agent and the minimum evaluation value for the Blue agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from referee.game.board import Board, IllegalActionException\n",
    "\n",
    "\n",
    "# def find_best_action(color, board, depth, maximizing_player):\n",
    "#     best_eval = float('-inf') if maximizing_player else float('inf')\n",
    "#     best_action = None\n",
    "\n",
    "#     for action in get_legal_actions(color, board):\n",
    "#         new_state = deepcopy(board)\n",
    "#         try:\n",
    "#             new_state.apply_action(action)\n",
    "#             if new_state._total_power <= 49:\n",
    "#                 eval = minimax_alpha_beta(new_state, depth - 1, float('-inf'), float('inf'), not maximizing_player)\n",
    "#                 if maximizing_player:\n",
    "#                     if eval > best_eval:\n",
    "#                         best_eval = eval\n",
    "#                         best_action = action\n",
    "#                 else:\n",
    "#                     if eval < best_eval:\n",
    "#                         best_eval = eval\n",
    "#                         best_action = action\n",
    "#         except IllegalActionException:\n",
    "#             pass\n",
    "\n",
    "#     return best_action\n",
    "def find_best_action(color, board, max_depth, time_limit, maximizing_player):\n",
    "    best_eval = float('-inf') if maximizing_player else float('inf')\n",
    "    best_action = None\n",
    "    start_time = time.time()\n",
    "\n",
    "    transposition_table = {}\n",
    "\n",
    "\n",
    "    for depth in range(1, max_depth + 1):\n",
    "        temp_best_eval = best_eval\n",
    "        temp_best_action = best_action\n",
    "\n",
    "        for action in get_legal_actions(color, board):\n",
    "            new_state = deepcopy(board)\n",
    "            try:\n",
    "                new_state.apply_action(action)\n",
    "                if new_state._total_power <= 49:\n",
    "                    eval = minimax_alpha_beta(new_state, depth - 1, float('-inf'), float('inf'), not maximizing_player, transposition_table, start_time, time_limit)\n",
    "                    \n",
    "                    if eval is None:  # Time limit exceeded\n",
    "                        break\n",
    "\n",
    "\n",
    "                    if maximizing_player:\n",
    "                        if eval > temp_best_eval:\n",
    "                            temp_best_eval = eval\n",
    "                            temp_best_action = action\n",
    "                    else:\n",
    "                        if eval < temp_best_eval:\n",
    "                            temp_best_eval = eval\n",
    "                            temp_best_action = action\n",
    "            except IllegalActionException:\n",
    "                pass\n",
    "\n",
    "        # If the time limit is exceeded, break out of the outer loop as well\n",
    "        if eval is None:\n",
    "            break\n",
    "\n",
    "        # Update best_eval and best_action if a better action was found\n",
    "        if maximizing_player and temp_best_eval > best_eval:\n",
    "            best_eval = temp_best_eval\n",
    "            best_action = temp_best_action\n",
    "        elif not maximizing_player and temp_best_eval < best_eval:\n",
    "            best_eval = temp_best_eval\n",
    "            best_action = temp_best_action\n",
    "\n",
    "        # Check if the allotted time has passed, break out of the loop if it has\n",
    "        current_time = time.time()\n",
    "        if current_time - start_time > time_limit:\n",
    "            break\n",
    "\n",
    "    return best_action\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Apply the best action: Update the game state by applying the best action found in the previous step for each agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: I am playing as blue\n",
      "                         ..     \n",
      "                     ..      ..     \n",
      "                 ..      ..      ..     \n",
      "             ..      ..      ..      ..     \n",
      "         ..      ..      ..      ..      ..     \n",
      "     ..      ..      ..      ..      ..      ..     \n",
      " ..      r1      ..      ..      ..      ..      ..     \n",
      "     ..      ..      ..      ..      ..      ..     \n",
      "         ..      ..      ..      ..      ..     \n",
      "             ..      ..      ..      ..     \n",
      "                 ..      ..      ..     \n",
      "                     ..      ..     \n",
      "                         ..     \n",
      "\n",
      "Testing: BLUE SPAWN at 5-1\n"
     ]
    }
   ],
   "source": [
    "from agent import Agent\n",
    "from referee.game import PlayerColor\n",
    "\n",
    "agentA = Agent(PlayerColor.BLUE)\n",
    "# Create an instance of your game state (assuming you have a Board class)\n",
    "board = my_board  # Replace this with the actual way to create an instance of your game state\n",
    "print(my_board.render())\n",
    "# Set the search depth and maximizing player\n",
    "search_depth = 3\n",
    "maximizing_player = True  # True for Red, False for Blue\n",
    "\n",
    "adaptive_search_depth = adaptive_depth(board, 3, 5)\n",
    "\n",
    "# Call the find_best_action function\n",
    "# best_action = find_best_action(board.turn_color, board, search_depth, maximizing_player)\n",
    "\n",
    "best_action = find_best_action(board.turn_color, board, max_depth=5, time_limit=2, maximizing_player=True)\n",
    "# Print the best action\n",
    "turnA = agentA.turn(PlayerColor.BLUE, best_action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************************************************************************\n",
      "Welcome to Infexion referee version 2023.0.1.\n",
      "\n",
      "Conduct a game of Infexion between 2 Agent classes.\n",
      "\n",
      "Run `python -m referee --help` for additional usage information.\n",
      "*******************************************************************************\n",
      "\u001b[37m* referee : \u001b[0mall messages printed by referee/wrapper modules begin with *\n",
      "\u001b[37m* referee : \u001b[0m(any other lines of output must be from your Agent class).\n",
      "\u001b[37m* referee : \u001b[0m\n",
      "\u001b[37m* referee : \u001b[0mwrapping player 1 [agent:Agent] as RED...\n",
      "\u001b[37m* referee : \u001b[0mwrapping player 2 [agent:Agent] as BLUE...\n",
      "\u001b[37m* referee : \u001b[0mlet the game begin!\n",
      "\u001b[37m* referee : \u001b[0mplayer RED is initialising\n",
      "Testing: I am playing as red\n",
      "\u001b[37m* referee : \u001b[0mplayer BLUE is initialising\n",
      "Testing: I am playing as blue\n",
      "\u001b[37m* referee : \u001b[0mRED to play (turn 1) ...\n",
      "\u001b[37m* referee : \u001b[0mRED plays action ACK\n",
      "\u001b[37m* referee ! \u001b[0mplayer error: ILLEGAL ACTION: Unknown action ACK\n",
      "\u001b[37m* referee : \u001b[0mgame over, winner is BLUE\n",
      "\u001b[37m* referee @ \u001b[0mresult: player 2 [agent:Agent]\n"
     ]
    }
   ],
   "source": [
    "!python3 -m referee agent agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '」' (U+300D) (1168922825.py, line 277)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [2], line 277\u001b[0;36m\u001b[0m\n\u001b[0;31m    return piece_diff 」\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character '」' (U+300D)\n"
     ]
    }
   ],
   "source": [
    "# COMP30024 Artificial Intelligence, Semester 1 2023\n",
    "# Project Part B: Game Playing Agent\n",
    "\n",
    "from referee.game import PlayerColor, Action, SpawnAction, SpreadAction, HexPos, HexDir, Board, IllegalActionException, \\\n",
    "    BOARD_N\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "# This is the entry point for your game playing agent. Currently, the agent\n",
    "# simply spawns a token at the centre of the board if playing as RED, and\n",
    "# spreads a token at the centre of the board if playing as BLUE. This is\n",
    "# intended to serve as an example of how to use the referee API -- obviously\n",
    "# this is not a valid strategy for actually playing the game!\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, color: PlayerColor, **referee: dict):\n",
    "        \"\"\"\n",
    "        Initialise the agent.\n",
    "        \"\"\"\n",
    "        self._color = color\n",
    "        self.board = Board()  # Create a new board instance\n",
    "        self.board.render()\n",
    "\n",
    "        match color:\n",
    "            case PlayerColor.RED:\n",
    "                print(\"Red Typhon is gonna destroy you!! ๐˙Ⱉ˙๐ \")\n",
    "            case PlayerColor.BLUE:\n",
    "                print(\"Blue Typhon is gonna destroy you!! (〃'▽'〃)\")\n",
    "\n",
    "    def action(self, **referee: dict) -> Action:\n",
    "        \"\"\"\n",
    "        Return the next action to take.\n",
    "        \"\"\"\n",
    "        # match self._color:\n",
    "        #     case PlayerColor.RED:\n",
    "        #         return SpawnAction(HexPos(3, 3))\n",
    "        #     case PlayerColor.BLUE:\n",
    "        #         # This is going to be invalid... BLUE never spawned!\n",
    "        #         return SpreadAction(HexPos(3, 3), HexDir.Up)\n",
    "        return minimax_decision(self.board)\n",
    "\n",
    "    def turn(self, color: PlayerColor, action: Action, **referee: dict):\n",
    "        \"\"\"\n",
    "        Update the agent with the last player's action.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.board.apply_action(action)\n",
    "        except IllegalActionException as e:\n",
    "            print(f\"Error: Illegal action '{action}' from player {color}: {e}\")\n",
    "\n",
    "        print(referee[\"time_remaining\"])\n",
    "        # Your previous code for printing actions\n",
    "        match action:\n",
    "            case SpawnAction(cell):\n",
    "                print(f\"Testing: {color} SPAWN at {cell}\")\n",
    "            case SpreadAction(cell, direction):\n",
    "                print(f\"Testing: {color} SPREAD from {cell}, {direction}\")\n",
    "\n",
    "\n",
    "def extract_positions(from_board: Board, color: Optional[PlayerColor] = None) -> [HexPos]:\n",
    "    \"\"\"\n",
    "    Extract empty or colored cells from the board\n",
    "    \"\"\"\n",
    "    return [hex_pos for hex_pos, cell_state in from_board._state.items()\n",
    "            if color is None and cell_state.player is None or\n",
    "            cell_state.player == color]\n",
    "\n",
    "\n",
    "def filter_cells(from_board):\n",
    "    team_color = from_board._turn_color\n",
    "    opponent_color = team_color.opponent\n",
    "\n",
    "    team_cells = []\n",
    "    opponent_cells = []\n",
    "    empty_cells = []\n",
    "\n",
    "    team_power = 0\n",
    "    opponent_power = 0\n",
    "\n",
    "    for hex_pos, cell_state in from_board._state.items():\n",
    "        if cell_state.player is team_color:\n",
    "            team_cells.append(hex_pos)\n",
    "            team_power += cell_state.power\n",
    "        elif cell_state.player is opponent_color:\n",
    "            opponent_cells.append(hex_pos)\n",
    "            opponent_power += cell_state.power\n",
    "        else:\n",
    "            empty_cells.append(hex_pos)\n",
    "\n",
    "    return team_cells, team_power, opponent_cells, opponent_power, empty_cells\n",
    "\n",
    "\n",
    "def get_neighbour_cells(from_board: Board, from_cells: [HexPos]):\n",
    "    \"\"\"\n",
    "    Get all cells covered by the spread action\n",
    "    \"\"\"\n",
    "    to_cells = [\n",
    "        from_cell + hex_dir * (i + 1)\n",
    "        for from_cell in from_cells\n",
    "        for hex_dir in HexDir\n",
    "        for i in range(from_board[from_cell].power)\n",
    "    ]\n",
    "    return list(set(to_cells))\n",
    "\n",
    "\n",
    "def get_spread_actions(from_board: Board) -> [SpreadAction]:\n",
    "    \"\"\"\n",
    "    Assign spread action in all direction to a cell and store them in a list\n",
    "    \"\"\"\n",
    "    color = from_board._turn_color\n",
    "\n",
    "    colored_positions = extract_positions(from_board, color)\n",
    "\n",
    "    spread_actions = [SpreadAction(position, hex_dir)\n",
    "                      for position in colored_positions\n",
    "                      for hex_dir in HexDir]\n",
    "\n",
    "    return spread_actions\n",
    "\n",
    "\n",
    "def get_spawn_actions(from_board: Board):\n",
    "    \"\"\"\n",
    "    Filter all empty cells on a board and match them to the spawn action\n",
    "    \"\"\"\n",
    "\n",
    "    color = from_board._turn_color\n",
    "\n",
    "    empty_positions = extract_positions(from_board)\n",
    "\n",
    "    spawn_actions = [SpawnAction(position)\n",
    "                     for position in empty_positions]\n",
    "\n",
    "    if len(from_board._history) < 24:\n",
    "        opponent_positions = extract_positions(from_board, color.opponent)\n",
    "        neighbour_cells = get_neighbour_cells(from_board, opponent_positions)\n",
    "\n",
    "        spawn_actions = [x for x in spawn_actions if x.cell not in neighbour_cells]\n",
    "\n",
    "    return spawn_actions\n",
    "\n",
    "\n",
    "def get_legal_actions(from_board: Board):\n",
    "    \"\"\"\n",
    "    Return all legal actions.\n",
    "    If total power exceed 49, return do not include spawn actions anymore\n",
    "    \"\"\"\n",
    "    spawn_actions = get_spawn_actions(from_board)\n",
    "    spread_actions = get_spread_actions(from_board)\n",
    "\n",
    "    if from_board._total_power >= 49:\n",
    "        return spread_actions\n",
    "    return spawn_actions + spread_actions\n",
    "\n",
    "\n",
    "def dominant(from_board: Board):\n",
    "    win_power_r = 0\n",
    "    win_piece_r = 0\n",
    "    win_power_q = 0\n",
    "    win_piece_q = 0\n",
    "\n",
    "    color = from_board._turn_color\n",
    "    opponent_color = color.opponent\n",
    "\n",
    "    for r in range(BOARD_N):\n",
    "        team_power_r = 0\n",
    "        team_power_q = 0\n",
    "        team_piece_r = 0\n",
    "        team_piece_q = 0\n",
    "\n",
    "        opponent_power_r = 0\n",
    "        opponent_power_q = 0\n",
    "        opponent_piece_r = 0\n",
    "        opponent_piece_q = 0\n",
    "        for q in range(BOARD_N):\n",
    "            current_cell = from_board._state[HexPos(q, r)]\n",
    "            if current_cell.player == color:\n",
    "                team_power_q += current_cell.power\n",
    "                team_piece_q += 1\n",
    "            elif current_cell.player == opponent_color:\n",
    "                opponent_power_q += current_cell.power\n",
    "                opponent_piece_q += 1\n",
    "\n",
    "            current_cell = from_board._state[HexPos(r, q)]\n",
    "            if current_cell.player == color:\n",
    "                team_power_r += current_cell.power\n",
    "                team_piece_r += 1\n",
    "            elif current_cell.player == opponent_color:\n",
    "                opponent_power_r += current_cell.power\n",
    "                opponent_piece_r += 1\n",
    "\n",
    "        if team_power_r > opponent_power_r:\n",
    "            win_power_r += 1\n",
    "        if team_piece_r > opponent_piece_r:\n",
    "            win_piece_r += 1\n",
    "        if team_power_q > opponent_power_q:\n",
    "            win_power_q += 1\n",
    "        if team_piece_q > opponent_piece_q:\n",
    "            win_piece_q += 1\n",
    "\n",
    "    return win_power_r, win_power_q, win_piece_r, win_piece_q\n",
    "\n",
    "\n",
    "def evaluate_power(from_board: Board):\n",
    "    color = from_board._turn_color\n",
    "    opponent_color = color.opponent\n",
    "\n",
    "    team_power = from_board._color_power(color)\n",
    "    opponent_power = from_board._color_power(opponent_color)\n",
    "\n",
    "    return team_power, team_power - opponent_power\n",
    "\n",
    "\n",
    "def evaluate_piece(from_board: Board):\n",
    "    team_tokens = extract_positions(from_board, from_board._turn_color)\n",
    "    opponent_tokens = extract_positions(from_board, from_board._turn_color.opponent)\n",
    "\n",
    "    team_tokens_num = len(team_tokens)\n",
    "    opponent_token_num = len(opponent_tokens)\n",
    "\n",
    "    piece_diff = team_tokens_num - opponent_token_num\n",
    "\n",
    "    vulnerable_tokens = [x for x in team_tokens if x in get_neighbour_cells(from_board, opponent_tokens)]\n",
    "    vulnerable_tokens_num = len(vulnerable_tokens)\n",
    "\n",
    "    aggressive_tokens = [y for y in opponent_tokens if y in get_neighbour_cells(from_board, team_tokens)]\n",
    "    aggressive_tokens_num = len(aggressive_tokens)\n",
    "\n",
    "    attack_benefit = aggressive_tokens_num - vulnerable_tokens_num\n",
    "\n",
    "    # risk_signal = False\n",
    "    # for token in team_tokens:\n",
    "    #     if board[token].power == 6:\n",
    "    #         risk_signal = True\n",
    "\n",
    "    return team_tokens_num, piece_diff, vulnerable_tokens_num, aggressive_tokens_num, attack_benefit\n",
    "\n",
    "\n",
    "def evaluate(from_board: Board, pre_power_diff):\n",
    "    # if from_board.game_over:\n",
    "    #     return 9999\n",
    "\n",
    "    # win_power_r, win_power_q, win_piece_r, win_piece_q = dominant(from_board)\n",
    "\n",
    "    team_cells, team_power, opponent_cells, opponent_power, empty_cells = filter_cells(from_board)\n",
    "\n",
    "    piece_diff = len(team_cells) - len(opponent_cells)\n",
    "    power_diff = team_power - opponent_power\n",
    "\n",
    "    # total_power, power_diff = evaluate_power(from_board)\n",
    "    # total_piece, piece_diff, vulnerable_tokens_num, aggressive_tokens_num, attack_benefit = evaluate_piece(from_board)\n",
    "    #\n",
    "    # # piece_gain = total_piece - pre_piece\n",
    "    # # power_gain = total_power - pre_power\n",
    "    # net_power_diff = abs(power_diff - pre_power_diff)\n",
    "    #\n",
    "    # win_piece_weight = 1\n",
    "    # win_power_weight = 1\n",
    "    # piece_gain_weight = 1\n",
    "    # power_gain_weight = 0.5\n",
    "    # power_diff_weight = 1\n",
    "    # piece_diff_weight = 1\n",
    "    # attack_benefit_weight = 3\n",
    "    # net_power_diff_weight = 5\n",
    "    #\n",
    "    # # if len(board._history) > 30:\n",
    "    # #     net_power_diff_weight = 4\n",
    "    #\n",
    "    # score = win_power_weight * (win_power_r + win_power_q) + \\\n",
    "    #         win_piece_weight * (win_piece_r + win_piece_q)\n",
    "    # power_diff_weight * power_diff + \\\n",
    "    # piece_diff_weight * piece_diff\n",
    "    # net_power_diff_weight * net_power_diff\n",
    "    # attack_benefit_weight * attack_benefit + \\\n",
    "    # piece_gain_weight * piece_gain + \\\n",
    "    # power_gain_weight * power_gain + \\\n",
    "\n",
    "    return piece_diff + power_diff\n",
    "\n",
    "\n",
    "def adaptive_depth(from_board):\n",
    "    turn = len(from_board._history)\n",
    "\n",
    "    return 2\n",
    "\n",
    "\n",
    "def minimax_decision(from_board: Board):\n",
    "    max_value = float('-inf')\n",
    "    best_action = None\n",
    "    depth = 2\n",
    "    is_maximizing = True\n",
    "    alpha = float('-inf')\n",
    "    beta = float('inf')\n",
    "\n",
    "    for action in get_legal_actions(from_board):\n",
    "        from_board.apply_action(action)\n",
    "        value = minimax_value(from_board, depth, alpha, beta, not is_maximizing, 0)\n",
    "        from_board.undo_action()\n",
    "\n",
    "        if value > max_value:\n",
    "            max_value = value\n",
    "            best_action = action\n",
    "\n",
    "    return best_action\n",
    "\n",
    "\n",
    "def minimax_value(from_board: Board, depth: int, alpha, beta, is_maximizing: bool, current_power_diff):\n",
    "    if depth == 0 or from_board.game_over:\n",
    "        return evaluate(from_board, current_power_diff)\n",
    "\n",
    "    legal_actions = get_legal_actions(from_board)\n",
    "\n",
    "    if is_maximizing:\n",
    "        max_value = float('-inf')\n",
    "\n",
    "        for action in legal_actions:\n",
    "            pre_power_diff = key_factor(from_board)\n",
    "\n",
    "            from_board.apply_action(action)\n",
    "            value = minimax_value(from_board, depth - 1, alpha, beta, not is_maximizing, pre_power_diff)\n",
    "            from_board.undo_action()\n",
    "\n",
    "            if value > max_value:\n",
    "                max_value = value\n",
    "\n",
    "            alpha = max(alpha, max_value)\n",
    "            if beta <= alpha:\n",
    "                break\n",
    "\n",
    "        return max_value\n",
    "\n",
    "    else:\n",
    "        min_value = float('inf')\n",
    "\n",
    "        for action in legal_actions:\n",
    "            pre_power_diff = key_factor(from_board)\n",
    "\n",
    "            from_board.apply_action(action)\n",
    "            value = minimax_value(from_board, depth - 1, alpha, beta, not is_maximizing, pre_power_diff)\n",
    "            from_board.undo_action()\n",
    "\n",
    "            if value < min_value:\n",
    "                min_value = value\n",
    "\n",
    "            beta = min(beta, min_value)\n",
    "            if beta <= alpha:\n",
    "                break\n",
    "\n",
    "        return min_value\n",
    "\n",
    "\n",
    "def key_factor(from_board: Board):\n",
    "    team_power = from_board._color_power(from_board._turn_color)\n",
    "    opponent_power = from_board._color_power(from_board._turn_color.opponent)\n",
    "\n",
    "    power_diff = team_power - opponent_power\n",
    "\n",
    "    return power_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
